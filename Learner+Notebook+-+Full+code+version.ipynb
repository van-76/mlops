{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Project 1: Pizza Sales Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Business Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A moderately sized, locally popular food joint operates a chain of outlets located in a metropolitan area, offering a diverse menu of pizzas, sides, and beverages. Despite having a steady flow of customers, they face challenges in optimizing their order fulfillment process, leading to delays during peak hours, which results in customer dissatisfaction and impacts repeat business. Additionally, they struggle with inventory management, often experiencing shortages of popular ingredients or excess stock of less favored items. To address these issues, they are implementing a new order management system and seeking to analyze sales data to better predict demand and streamline inventory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Objective**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "You have been engaged by the business as a Data Analyst to enhance operational efficiency and boost customer satisfaction. You have been provided with raw historical sales data and tasked with pre-processing historical sales data to uncover trends, building an interactive dashboard to enable visual reporting of key metrics, and generating email reports to communicate key insights to stakeholders. This will enable the stakeholders to get a clearer understanding of the business, stay on top of changing market scenarios via frequent alerts, and make quick, informed decisions to resolve operational challenges. The anticipated outcomes include reduced order processing times, improved inventory turnover, and increased customer satisfaction leading to higher repeat sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Data Description:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This dataset contain detailed information about pizza orders, including specifics about the pizza variants, quantities, pricing, dates, times, and categorization details.\n",
        "- **pizza_id:** A unique identifier assigned to each distinct pizza variant available for ordering.  \n",
        "- **order_id:** A unique identifier for each order made, which links to multiple pizzas.  \n",
        "- **pizza_name_id:** An identifier linking to a specific name of the pizza.  \n",
        "- **quantity:** The number of units of a specific pizza variant ordered within an order.  \n",
        "- **order_date:** The date when the order was placed.  \n",
        "- **order_time:** The time when the order was placed.  \n",
        "- **unit_price:** The cost of a single unit of the specific pizza variant.  \n",
        "- **pizza_size:** Represents the size of the pizza (e.g., small, medium, large).  \n",
        "- **pizza_category:** Indicates the category of the pizza, such as vegetarian, non-vegetarian, etc.  \n",
        "- **pizza_name:** Specifies the name of the specific pizza variant ordered.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **AzureML Setup and Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Connect to Azure Machine Learning Workspace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1728104747643
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "credential = DefaultAzureCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1728104806774
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"REPLACE WITH YOUR SUBSCRIPTION ID\",\n",
        "    resource_group_name=\"REPLACE WITH YOUR RESOURCE GROUP\",\n",
        "    workspace_name=\"REPLACE WITH YOUR WORKSPACE NAME\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Create Compute Cluster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1728104839082
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"Standard_A2_v2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster).result()\n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Register Dataset into Data Assets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1728104858058
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Create a Job Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1728104877486
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a directory called \"project_env\" for the preprocessing script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a code.yml with all the dependencies and store it in to the project_env directorty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1728104880767
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the environment with the code.yml file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Data Overview**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1728104946122
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment one of the following code snippets and execute it to install the seaborn library\n",
        "# !pip install seaborn  \n",
        "# pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1728104957016
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1728104965676
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1728104967727
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# print the top 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1728104968582
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# get the shape of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1728104969040
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# get the info of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1728104969570
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# generate the statistical summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1728104970225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# calculate the sum of duplicated values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1728104976473
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# check for any missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1728104977113
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Univariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1728104987500
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code to convert the order_date and order_time columns into a single order_time column in datetime format, and then drop the original order_date column from the dataframe. Finally, display the first few rows of the modified dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1728104998935
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code snippet to extract the month from the order_time column and create a new column called order_month. Additionally, classify the order_time into different parts of the day (Morning, Afternoon, Evening) based on the hour and store this information in a new column called time_of_day. Finally, display the first few rows of the modified dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1728104999421
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1728105005656
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1728105009556
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a directory called \"project_scripts\" for the preprocessing script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Create Preprocessing Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile {src_dir_job_scripts}/pre_process.py\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--output\", type=str, help=\"path to output data\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Read the input data from the specified path\n",
        "    df = pd.read_csv(args.data)\n",
        "    \n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    \n",
        "    #Keep all data processing and feature engineering \n",
        "    #steps here to automate the entire feature engineering process\n",
        "    \n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "    # Define output paths\n",
        "    output_data_store = args.output\n",
        "    preprocessed_data_output_path = Path(output_data_store, \"processed_data.csv\")\n",
        "\n",
        "    # Save the processed data\n",
        "    df.to_csv(preprocessed_data_output_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Creating Preprocessing Job**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code snippet to define a data preparation step for a pizza analysis project using a command function. The step should read a CSV input, process and transform the data, with inputs and outputs specified as uri_folder. Include a description, display name, and the command to execute a Python script for data preprocessing. Use a specified environment for execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1728105075905
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Reporting via Dashboard**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Creating Files for the Dashboard**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Creating Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1728105078495
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a directory called project_files for storing the dashboard files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Creating Dashboard Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile {src_dir_hf_space}/app.py\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "def load_data():\n",
        "    df = pd.read_csv(\"processed_data.csv\")  # replace with your dataset\n",
        "    return df\n",
        "\n",
        "# Create Streamlit app\n",
        "def app():\n",
        "    # Title for the app\n",
        "    st.title(\"Pizza Sales Data Analysis Dashboard\")\n",
        "    df = load_data()\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    # Calculate key metrics\n",
        "    # Write a code snippet to calculate key metrics from the pizza orders dataframe, including the \n",
        "    # total number of unique orders, total revenue generated, the most popular pizza size, the most \n",
        "    # frequent pizza category, total pizzas sold\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "    # Sidebar with key metrics\n",
        "    # Write a code snippet to display key metrics in the sidebar of a Streamlit application. \n",
        "    # Show the total number of orders, total revenue (formatted as currency), the most popular\n",
        "    # pizza size, the most popular pizza category, and the total number of pizzas sold \n",
        "    # using the st.sidebar.metric function.\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "    # Provide the details of the plots here\n",
        "    plots = [\n",
        "        {\"title\": \"__________\", \"x\": \"_________\", \"y\": \"___________\"},\n",
        "    ]\n",
        "\n",
        "    for plot in plots:\n",
        "      st.header(plot[\"title\"])\n",
        "      \n",
        "      fig, ax = plt.subplots()\n",
        "      \n",
        "\n",
        "      \"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "      # Provide the details of the plots here\n",
        "\n",
        "\n",
        "      \"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "      st.pyplot(fig)\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Creating Requirements File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here and store in to the project_files directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating Docker File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile {src_dir_hf_space}/Dockerfile\n",
        "FROM python:3.9-slim\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "\n",
        "USER user\n",
        "\n",
        "ENV HOME=/home/user \\\n",
        "\n",
        "    PATH=/home/user/.local/bin:$PATH\n",
        "\n",
        "WORKDIR $HOME/app\n",
        "\n",
        "COPY --chown=user . $HOME/app\n",
        "\n",
        "\n",
        "\n",
        "# Run apt-get update and install as root\n",
        "USER root\n",
        "\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    build-essential \\\n",
        "\n",
        "    curl \\\n",
        "\n",
        "    software-properties-common \\\n",
        "\n",
        "    git \\\n",
        "\n",
        "    && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "USER user\n",
        "\n",
        "\n",
        "# COPY requirements.txt ./\n",
        "\n",
        "# COPY src/ ./src/\n",
        "\n",
        "COPY . .\n",
        "\n",
        "RUN pip3 install -r requirements.txt\n",
        "\n",
        "EXPOSE 8501\n",
        "\n",
        "HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n",
        "\n",
        "ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Creating Script to Push Files to Hugging Face Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile {src_dir_job_scripts}/hugging_face_auth_push_files.py\n",
        "from huggingface_hub import login, HfApi\n",
        "from datasets import Dataset\n",
        "import argparse\n",
        "import os \n",
        "import pandas as pd\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True) # Create the \"outputs\" directory if it doesn't exist\n",
        "\n",
        "\n",
        "# Once the job 1 run, the output processed_data.csv will gonna store under a output folder that we created below \n",
        "# After the job 1 completes, we can read the processed_data.csv file in the job 2 \n",
        "\n",
        "def select_first_file(path):\n",
        "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
        "    Args:\n",
        "        path (str): path to directory or file to choose\n",
        "    Returns:\n",
        "        str: full path of selected file\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Provide the input arguments for these job\n",
        "    parser.add_argument(\"--processed_data_push\", type=str, help=\"path to processed data\")\n",
        "    parser.add_argument(\"--streamlit_files\", type=str, help=\"path to streamlit files\")\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Write a code snippet to authenticate and upload files to a Hugging Face space. \n",
        "    # The snippet should include logging in using an access token, initializing the Hugging Face API, \n",
        "    # uploading a folder containing application files and a requirements file, and \n",
        "    # then uploading a specific processed data file as processed_data.csv\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Creating Dashboard Reporting Job**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code to define a Hugging Face authentication step for a dashboard project using a command function. The step should authenticate to the Hugging Face hub and push specified files. It should include inputs for the path to the processed data and a folder containing Streamlit files, along with the command to execute a Python script for authentication and file upload. Specify the source folder for the component and the environment for execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1728105289506
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Reporting via Email**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Creating Email Report Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1727286024483
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile {src_dir_job_scripts}/daily_email_report.py\n",
        "import argparse\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "os.makedirs(\"./outputs\", exist_ok=True) # Create the \"outputs\" directory if it doesn't exist\n",
        "\n",
        "\n",
        "def select_first_file(path):\n",
        "    files = os.listdir(path)\n",
        "    return os.path.join(path, files[0])\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "\n",
        "# Define email configuration with sender, receiver, app passwords along with port number\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--processed_data\", type=str, help=\"path to processed data\")\n",
        "    args = parser.parse_args()\n",
        "    df = pd.read_csv(select_first_file(args.processed_data))  # Read the processed data\n",
        "\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "    # Calculate the top 5 metrics\n",
        "    # Write a code snippet to calculate key metrics from a pizza orders dataframe. \n",
        "    # This should include the total number of unique orders, total revenue from orders, \n",
        "    # the most popular pizza size, the most frequent pizza category, and the total quantity\n",
        "    # of pizzas sold.Each metric should be clearly defined with a brief comment explaining its purpose.\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "    # Create a report dictionary for summary\n",
        "\n",
        "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "    # Create email content\n",
        "    def create_email_content(report):\n",
        "        email_content = f\"\"\"\n",
        "\n",
        "\n",
        "        #Write you are message here\n",
        "\n",
        "        \"\"\"\n",
        "        return email_content\n",
        "\n",
        "    # Send email report\n",
        "    def send_email_report(email_config, report_metrics):\n",
        "        email_content = create_email_content(report_metrics)\n",
        "\n",
        "        server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])\n",
        "        server.starttls()\n",
        "        server.login(email_config['sender_email'], email_config['password'])\n",
        "\n",
        "        msg = MIMEMultipart()  # Create new MIMEMultipart object for each recipient\n",
        "        msg['From'] = email_config['sender_email']\n",
        "        msg['To'] = email_config['receiver_emails']\n",
        "        msg['Subject'] = 'Pizza Sales Analysis Report'\n",
        "        msg.attach(MIMEText(email_content, 'html'))\n",
        "\n",
        "        text = msg.as_string()\n",
        "        server.sendmail(email_config['sender_email'], email_config['receiver_emails'], text)\n",
        "        print(f\"Email report sent to {email_config['receiver_emails']} successfully!\")\n",
        "\n",
        "        server.quit()\n",
        "\n",
        "    # Call the function to send email report\n",
        "    send_email_report(email_config, report)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Creating Email Reporting Job**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code to define a step for sending report emails based on processed pizza order data. The step should include inputs for the processed data (as a URI folder), a clear name and display name, and a description of its functionality. Specify the source folder for the component and the command to execute a Python script that generates and sends the email report. Use the designated environment for execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1728105313931
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write you are code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Building an Analytical Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Assembling all Jobs into a Single Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code snippet to define an Azure ML pipeline for the Intelligent Reporting Project. The pipeline should include three steps: \n",
        "\n",
        "1. A data preparation job that processes an input CSV file (pizza_sales.csv).\n",
        "2. A job for Hugging Face authentication and pushing files, taking the processed data from the first job and specified Streamlit files as inputs.\n",
        "3. An email report job that sends a report based on the processed data from the first job.\n",
        "\n",
        "Ensure that the pipeline has appropriate input parameters and returns a dictionary of outputs with identifiers for each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Providing Paths for the Jobs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1728105316850
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define paths for input registered data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Write a code snippet to instantiate the defined Azure ML pipeline by providing the necessary input parameters. The pipeline should take the input path for the pizza sales data as a URI file and a URI folder for the Hugging Face files, ensuring that both inputs are correctly specified for execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1728105321404
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Executing the Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write a code snippet to create or update a job in Azure ML using the defined pipeline. The job should be associated with a specified experiment name, such as \"Project 1 - Intelligent Reporting on Azure ML Pipeline,\" and utilize the previously instantiated pipeline for execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Sample Output**\n",
        "\n",
        "1. Conclusions and Recommendations\n",
        "2. Hugging face space link and screenshot of dashboarding\n",
        "3. Email report screenshot "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Power Ahead!"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
